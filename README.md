# ğŸ§  AI Customer Support Agent (RAG + LLM + Redis + Ticketing + Monitoring)


Welcome to the **AI Customer Support Agent**, a production-ready backend system that demonstrates advanced AI workflows with **Retrieval-Augmented Generation (RAG)**, LLM reasoning, caching, ticketing, and monitoring. Its a customer support system that automatically answers user queries, tracks unanswered questions via a ticketing system, and provides real-time analytics through a dashboard. Demonstrates caching, context-aware responses, and monitoring to ensure reliable performance and actionable insights.

---

## âœ¨ Features

- **Semantic Search**: Retrieve relevant context from PDFs, FAQs, and knowledge base using **ChromaDB / FAISS**
- **LLM Agent**: Generate intelligent responses using **Ollama or OpenAI**
- **Redis Memory & Cache**: Speed up responses and maintain per-user context
- **Ticketing System**: Auto-create tickets for unanswered or low-confidence queries
- **Analytics Dashboard**: Track queries, cache hits, tickets, and visualize metrics
- **Monitoring**: **Prometheus + Grafana** integration for real-time metrics
- **Dockerized**: Easy setup and deployment across environments

---

## ğŸš€ Quick Setup

### 1ï¸âƒ£ Clone Repository
```bash
git clone [https://github.com/vickytilotia/AI_Support_Agent.git](https://github.com/vickytilotia/AI_Support_Agent.git)
cd AI-Support-Agent
```

### 2ï¸âƒ£ Python Environment
```bash
python -m venv venv
# Linux/Mac
source venv/bin/activate
# Windows
venv\Scripts\activate

pip install -r requirements.txt
```

### 3ï¸âƒ£ Docker Setup
```bash
docker-compose up --build
```
Access services:
- FastAPI â†’ [http://localhost:8000](http://localhost:8000)
- Prometheus â†’ [http://localhost:9090](http://localhost:9090)
- Grafana â†’ [http://localhost:3000](http://localhost:3000) (admin/admin)

### 4ï¸âƒ£ Initialize Database
Tables auto-create on FastAPI startup.
```bash
uvicorn app.main:app --reload
```

### 5ï¸âƒ£ Test Endpoints
- **Chat**: POST `/chat/` â†’ Send user query
- **Analytics**: GET `/analytics/` â†’ View system metrics

---

## ğŸ“Š Project Flow

```text
User Query
   â”‚
   â–¼
ChromaDB / FAISS (RAG) â†’ Retrieve context
   â”‚
   â–¼
LLM Agent â†’ Generate Answer
   â”‚
   â”œâ”€> Redis Cache â†’ Check / Store
   â””â”€> If Low Confidence â†’ Create Ticket (SQLite)
   â”‚
   â–¼
Update Analytics â†’ Prometheus Scrapes â†’ Grafana Visualizes
```
*Flowchart image: `flowchart.png`*

---

## ğŸ–¼ï¸ Screenshots (Placeholders)

| Scenario | Screenshot |
|----------|------------|
| Query & Answer | <img width="1476" height="671" alt="query answer" src="https://github.com/user-attachments/assets/375eb78f-fb1c-423b-ad2a-0e6cbe013539" /> |
| Query & Answer from Cache | <img width="1480" height="622" alt="query answer from cache" src="https://github.com/user-attachments/assets/e8c18027-ee6a-4873-af51-1b4c7e4e6cb6" />  |
| Ticket Creation | <img width="1479" height="692" alt="creating ticket " src="https://github.com/user-attachments/assets/990ae23f-8c0a-47f7-a148-84dd9514e60e" />  |


---

## ğŸ“ Folder Structure
```
AI-Support-Agent/
â”œâ”€ app/
â”‚  â”œâ”€ api/            # FastAPI routes
â”‚  â”œâ”€ services/       # LLM, RAG, Memory, Ticket, Analytics
â”‚  â”œâ”€ core/           # DB and Redis setup
â”‚  â”œâ”€ main.py         # FastAPI entrypoint
â”œâ”€ data/              # SQLite DB
â”œâ”€ requirements.txt
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ prometheus.yml
â”œâ”€ flowchart.png      # Visual project flow
â”œâ”€ README.md          # Project documentation
```

---

## ğŸŒŸ Next Steps / Enhancements
- Add **API authentication** (API keys / OAuth2)
- Implement **Celery async tasks** for ticket notifications
- Build **frontend dashboard** for queries and analytics
- Enhance **Grafana dashboards** with user-level metrics
- Demonstrate **multi-instance scaling** with Docker Compose

---

## ğŸ“ License
MIT License
